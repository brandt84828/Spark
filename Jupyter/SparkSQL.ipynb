{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a7aa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, ArrayType\n",
    "from pyspark.sql import functions as F\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d8a7ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|                time|             content|\n",
      "+--------------------+--------------------+\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create SparkSession\n",
    "spark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "#get SparkContext by SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "#Hello World\n",
    "df = spark.read.csv(\"logstash-2022.04.27.csv\",sep=',',header=False)\n",
    "df2 = df.toDF(\"time\",\"content\")\n",
    "df2.printSchema() # print table structure\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eecc32a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                time|             content|\n",
      "+--------------------+--------------------+\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SQL\n",
    "df2.createTempView(\"log\")\n",
    "spark.sql(\"\"\"SELECT * FROM log LIMIT 3 \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b60d3fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                time|             content|\n",
      "+--------------------+--------------------+\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DSL\n",
    "df2.where(\"time='2022-04-27T06:59:42.575644409+00:00'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a025eb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RDD to DataFrame 1\n",
    "rdd = sc.textFile(\"people.txt\").\\\n",
    "    map(lambda x: x.split(\",\")).\\\n",
    "    map(lambda x: (x[0],int(x[1])))\n",
    "\n",
    "#convert to DataFrame\n",
    "#para1: 被轉換的RDD\n",
    "#para2: 欄位名稱，透過list方式指定，按照順序提供\n",
    "df = spark.createDataFrame(rdd, schema=['name','age'])\n",
    "\n",
    "#輸出表結構\n",
    "df.printSchema()\n",
    "\n",
    "#輸出df中的數據\n",
    "#para1: 輸出筆數，default=20\n",
    "#para2: 是否截斷，如果數據長度超過20後續的內容是否顯示，True->截斷，False->不截斷\n",
    "df.show()\n",
    "\n",
    "#將df對象轉換成臨時視圖表，可供SQL語句查詢\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "spark.sql(\"SELECT * FROM people WHERE age < 30\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b71a36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = false)\n",
      "\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RDD to DataFrame 2 (StructType)\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType\n",
    "\n",
    "rdd = sc.textFile(\"people.txt\").\\\n",
    "    map(lambda x: x.split(\",\")).\\\n",
    "    map(lambda x: (x[0],int(x[1])))\n",
    "\n",
    "schema = StructType().add(\"name\",StringType(),nullable=True).\\\n",
    "            add(\"age\",IntegerType(),nullable=False)\n",
    "\n",
    "df = spark.createDataFrame(rdd, schema=schema)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bded5206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = false)\n",
      "\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RDD to DataFrame 3 (toDF)\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType\n",
    "\n",
    "rdd = sc.textFile(\"people.txt\").\\\n",
    "    map(lambda x: x.split(\",\")).\\\n",
    "    map(lambda x: (x[0],int(x[1])))\n",
    "\n",
    "\n",
    "#直接放List給欄位名稱這種方式類型只能靠推斷，對類型不敏感要快速創建使用\n",
    "df1 = rdd.toDF([\"name\",\"age\"])\n",
    "df1.printSchema()\n",
    "df1.show()\n",
    "\n",
    "#透過StructType才可以指定Type & Name\n",
    "schema = StructType().add(\"name\",StringType(),nullable=True).\\\n",
    "            add(\"age\",IntegerType(),nullable=False)\n",
    "df2 = rdd.toDF(schema)\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8873c33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n",
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1|  Amy| 11|\n",
      "|  2|  Bob| 21|\n",
      "|  3|Chris| 11|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RDD to DataFrame 4 (基於Pandas的DataFrame)\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType\n",
    "import pandas as pd\n",
    "\n",
    "pdf = pd.DataFrame(\n",
    "    {\n",
    "        \"id\":[1,2,3],\n",
    "        \"name\": [\"Amy\",\"Bob\",\"Chris\"],\n",
    "        \"age\": [11, 21, 11]\n",
    "    }\n",
    ")\n",
    "\n",
    "df = spark.createDataFrame(pdf)\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d2824d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: string (nullable = true)\n",
      "\n",
      "+-----------+\n",
      "|       data|\n",
      "+-----------+\n",
      "|Michael, 29|\n",
      "|   Andy, 30|\n",
      "| Justin, 19|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 統一API讀取\n",
    "#create SparkSession\n",
    "spark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "#get SparkContext by SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "#對於text是直接讀取整個列當作一筆資料，默認的欄位名稱是value類型是string\n",
    "schema = StructType().add(\"data\",StringType(),nullable=True)\n",
    "df = spark.read.format(\"text\").schema(schema).load(\"people.txt\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "159d767f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#JSON自帶schema訊息\n",
    "df = spark.read.format(\"json\").load(\"people.json\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37ebff65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|                time|             content|\n",
      "+--------------------+--------------------+\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "|2022-04-27T06:59:...|\"pattern not matc...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"sep\",\",\").option(\"header\",True).option(\"encoding\",\"utf-8\").schema(\"time STRING,content STRING\").load(\"logstash-2022.04.27.csv\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ee262",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"users.parquet\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1710984b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|subject|\n",
      "+---+-------+\n",
      "|  1|English|\n",
      "|  2|   Math|\n",
      "+---+-------+\n",
      "\n",
      "+---+-------+\n",
      "| id|subject|\n",
      "+---+-------+\n",
      "|  1|English|\n",
      "|  2|   Math|\n",
      "+---+-------+\n",
      "\n",
      "+---+-------+\n",
      "| id|subject|\n",
      "+---+-------+\n",
      "|  1|English|\n",
      "|  2|   Math|\n",
      "+---+-------+\n",
      "\n",
      "+---+-------+-----+\n",
      "| id|subject|score|\n",
      "+---+-------+-----+\n",
      "|  1|English|   90|\n",
      "|  2|   Math|   95|\n",
      "+---+-------+-----+\n",
      "\n",
      "+---+-------+-----+\n",
      "| id|subject|score|\n",
      "+---+-------+-----+\n",
      "|  1|English|   90|\n",
      "|  2|   Math|   95|\n",
      "+---+-------+-----+\n",
      "\n",
      "+---+-------+-----+\n",
      "| id|subject|score|\n",
      "+---+-------+-----+\n",
      "|  1|English|   90|\n",
      "|  2|   Math|   95|\n",
      "+---+-------+-----+\n",
      "\n",
      "+---+-------+-----+\n",
      "| id|subject|score|\n",
      "+---+-------+-----+\n",
      "|  1|English|   90|\n",
      "|  2|   Math|   95|\n",
      "+---+-------+-----+\n",
      "\n",
      "+-------+-----+\n",
      "|subject|count|\n",
      "+-------+-----+\n",
      "|   Math|    1|\n",
      "|English|    1|\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----+\n",
      "|subject|count|\n",
      "+-------+-----+\n",
      "|   Math|    1|\n",
      "|English|    1|\n",
      "+-------+-----+\n",
      "\n",
      "<class 'pyspark.sql.group.GroupedData'>\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").schema(\"id INT, subject STRING, score INT\").load(\"stu_score.txt\")\n",
    "\n",
    "# get column\n",
    "id_column = df[\"id\"]\n",
    "subject_column = df[\"subject\"]\n",
    "\n",
    "#DSL\n",
    "df.select([\"id\",\"subject\"]).show()\n",
    "df.select(\"id\",\"subject\").show()\n",
    "df.select(id_column,subject_column).show()\n",
    "\n",
    "#filter API\n",
    "df.filter(\"score < 99\").show()\n",
    "df.filter(df['score'] < 99).show()\n",
    "\n",
    "#where API\n",
    "df.where(\"score < 99\").show()\n",
    "df.where(df['score'] < 99).show()\n",
    "\n",
    "#groupby API\n",
    "df.groupBy(\"subject\").count().show()\n",
    "df.groupBy(df['subject']).count().show()\n",
    "\n",
    "# 回傳值是一個有分組關係的數據結構(GroupedData)不是DataFrame，調用聚合方法後回傳值還是DataFrame\n",
    "r = df.groupBy(\"subject\")\n",
    "print(type(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "181a7a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|subject|cnt|\n",
      "+-------+---+\n",
      "|   Math|  1|\n",
      "|English|  1|\n",
      "+-------+---+\n",
      "\n",
      "+-------+---+\n",
      "|subject|cnt|\n",
      "+-------+---+\n",
      "|   Math|  1|\n",
      "|English|  1|\n",
      "+-------+---+\n",
      "\n",
      "+-------+---+\n",
      "|subject|cnt|\n",
      "+-------+---+\n",
      "|   Math|  1|\n",
      "|English|  1|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").schema(\"id INT, subject STRING, score INT\").load(\"stu_score.txt\")\n",
    "\n",
    "#SQL\n",
    "df.createTempView(\"score\")\n",
    "df.createOrReplaceTempView(\"score2\")\n",
    "df.createGlobalTempView(\"score3\") \n",
    "\n",
    "spark.sql(\"SELECT subject, COUNT(*) AS cnt FROM score GROUP BY subject\").show()\n",
    "spark.sql(\"SELECT subject, COUNT(*) AS cnt FROM score2 GROUP BY subject\").show()\n",
    "spark.sql(\"SELECT subject, COUNT(*) AS cnt FROM global_temp.score3 GROUP BY subject\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d593fa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  word|cnt|\n",
      "+------+---+\n",
      "| hello|  3|\n",
      "| spark|  1|\n",
      "|hadoop|  1|\n",
      "| flink|  1|\n",
      "+------+---+\n",
      "\n",
      "+------+---+\n",
      "|  word|cnt|\n",
      "+------+---+\n",
      "| hello|  3|\n",
      "| spark|  1|\n",
      "|hadoop|  1|\n",
      "| flink|  1|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.textFile(\"words.txt\").flatMap(lambda x: x.split(\" \")).map(lambda x: [x])\n",
    "df = rdd.toDF([\"word\"])\n",
    "\n",
    "#SQL\n",
    "df.createOrReplaceTempView(\"words\")\n",
    "spark.sql(\"SELECT word, count(*) AS cnt FROM words GROUP BY word ORDER BY cnt DESC\").show()\n",
    "\n",
    "#DSL\n",
    "from pyspark.sql import functions as F\n",
    "df = spark.read.format(\"text\").load(\"words.txt\")\n",
    "\n",
    "#withColumn 對column操作，有更新成新列保留沒有就取代掉舊的\n",
    "df2 = df.withColumn(\"value\", F.explode(F.split(df['value'],\" \")))\n",
    "df2.groupBy(\"value\").count().withColumnRenamed(\"value\",\"word\").withColumnRenamed(\"count\",\"cnt\").orderBy(\"cnt\",ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00d069dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|user_id|avg_rank|\n",
      "+-------+--------+\n",
      "|    849|    4.87|\n",
      "|    688|    4.83|\n",
      "|    507|    4.72|\n",
      "|    628|     4.7|\n",
      "|    928|    4.69|\n",
      "|    118|    4.66|\n",
      "|    907|    4.57|\n",
      "|    686|    4.56|\n",
      "|    427|    4.55|\n",
      "|    565|    4.54|\n",
      "|    850|    4.53|\n",
      "|    469|    4.53|\n",
      "|    225|    4.52|\n",
      "|    330|     4.5|\n",
      "|    477|    4.46|\n",
      "|    242|    4.45|\n",
      "|    636|    4.45|\n",
      "|    583|    4.44|\n",
      "|    767|    4.43|\n",
      "|    252|    4.43|\n",
      "+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+--------+\n",
      "|movie_id|avg_rank|\n",
      "+--------+--------+\n",
      "|    1467|     5.0|\n",
      "|     814|     5.0|\n",
      "|    1653|     5.0|\n",
      "|    1536|     5.0|\n",
      "|    1293|     5.0|\n",
      "|    1122|     5.0|\n",
      "|    1201|     5.0|\n",
      "|    1189|     5.0|\n",
      "|    1599|     5.0|\n",
      "|    1500|     5.0|\n",
      "|    1449|    4.63|\n",
      "|    1398|     4.5|\n",
      "|    1642|     4.5|\n",
      "|    1594|     4.5|\n",
      "|     119|     4.5|\n",
      "|     408|    4.49|\n",
      "|     318|    4.47|\n",
      "|     169|    4.47|\n",
      "|     483|    4.46|\n",
      "|     114|    4.45|\n",
      "+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "大於平均分數電影的評分數量: 55375\n",
      "+-------------------+\n",
      "|round(avg(rank), 2)|\n",
      "+-------------------+\n",
      "|               3.86|\n",
      "+-------------------+\n",
      "\n",
      "+-------+--------+--------+--------+\n",
      "|user_id|avg_rank|min_rank|max_rank|\n",
      "+-------+--------+--------+--------+\n",
      "|    296|    4.18|       1|       5|\n",
      "|    467|    3.68|       2|       5|\n",
      "|    691|    4.22|       1|       5|\n",
      "|    675|    3.71|       1|       5|\n",
      "|    829|    3.55|       1|       5|\n",
      "|    125|    3.44|       1|       5|\n",
      "|    451|    2.73|       1|       5|\n",
      "|    800|    3.75|       2|       5|\n",
      "|    853|    2.98|       1|       5|\n",
      "|    666|    3.67|       2|       5|\n",
      "|    870|    3.45|       1|       5|\n",
      "|    919|    3.47|       1|       5|\n",
      "|    926|     3.3|       1|       5|\n",
      "|      7|    3.97|       1|       5|\n",
      "|    124|     3.5|       1|       5|\n",
      "|     51|    3.57|       1|       5|\n",
      "|    447|     3.6|       1|       5|\n",
      "|    591|    3.65|       2|       5|\n",
      "|    307|    3.79|       1|       5|\n",
      "|    475|     3.6|       1|       5|\n",
      "+-------+--------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+---+--------+\n",
      "|movie_id|cnt|avg_rank|\n",
      "+--------+---+--------+\n",
      "|     408|112|    4.49|\n",
      "|     318|298|    4.47|\n",
      "|     169|118|    4.47|\n",
      "|     483|243|    4.46|\n",
      "|      64|283|    4.45|\n",
      "|      12|267|    4.39|\n",
      "|     603|209|    4.39|\n",
      "|      50|583|    4.36|\n",
      "|     178|125|    4.34|\n",
      "|     357|264|    4.29|\n",
      "+--------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType().add(\"user_id\",StringType(),nullable=False).\\\n",
    "    add(\"movie_id\",IntegerType(),nullable=False).\\\n",
    "    add(\"rank\",IntegerType(),nullable=False).\\\n",
    "    add(\"ts\",StringType(),nullable=False)\n",
    "df = spark.read.format(\"csv\").option(\"sep\",\"\\t\").option(\"header\",False).option(\"encoding\",\"utf-8\").schema(schema).load(\"/home/cris/ml-100k/u.data\")\n",
    "\n",
    "# 用戶平均分\n",
    "df.groupBy(\"user_id\").avg(\"rank\").withColumnRenamed(\"avg(rank)\",\"avg_rank\").withColumn(\"avg_rank\",F.round(\"avg_rank\",2)).orderBy(\"avg_rank\",ascending=False).show()\n",
    "\n",
    "# 電影平均分\n",
    "df.createOrReplaceTempView(\"movie\")\n",
    "spark.sql(\"SELECT movie_id, ROUND(AVG(rank),2) AS avg_rank FROM movie GROUP BY movie_id ORDER BY avg_rank DESC\").show()\n",
    "\n",
    "# 查詢大於平均分數電影的評分數量\n",
    "print(\"大於平均分數電影的評分數量:\", df.where(df['rank'] > df.select(F.avg(df['rank'])).first()['avg(rank)']).count())\n",
    "\n",
    "# 查詢高分電影(分數>3)評分次數最多的用戶，他的平均評分\n",
    "# 找到此人 first>get first row\n",
    "user_id = df.where(\"rank > 3\").groupBy(\"user_id\").count().withColumnRenamed(\"count\",\"cnt\").orderBy(\"cnt\",ascending=False).limit(1).first()['user_id']\n",
    "\n",
    "# 計算這個人的平均評分\n",
    "df.filter(df['user_id'] == user_id).select(F.round(F.avg(\"rank\"),2)).show()\n",
    "\n",
    "# 查詢每個用戶的平均評分,最高評分,最低評分\n",
    "df.groupBy(\"user_id\").\\\n",
    "    agg(\n",
    "        F.round(F.avg(\"rank\"),2).alias(\"avg_rank\"),\n",
    "        F.min(\"rank\").alias(\"min_rank\"),\n",
    "        F.max(\"rank\").alias(\"max_rank\")\n",
    "    ).show()\n",
    "\n",
    "# 查詢評分超過100次的電影 平均評分排名TOP10的\n",
    "df.groupBy(\"movie_id\").\\\n",
    "    agg(\n",
    "        F.count(\"movie_id\").alias(\"cnt\"),\n",
    "        F.round(F.avg(\"rank\"),2).alias(\"avg_rank\")\n",
    "    ).where(\"cnt > 100\").\\\n",
    "    orderBy(\"avg_rank\",ascending=False).\\\n",
    "    limit(10).\\\n",
    "    show()\n",
    "\n",
    "# agg: GroupedData對象的API，作用是可以在裡面寫多個聚合\n",
    "# alias: Column對象的API，針對單一個欄位名稱改名\n",
    "# withColumnRenamed: DataFrame對象的API，可以對DF中的欄位進行改名，一次可以改1~多\n",
    "# orderBy: DataFrame對象的API，進行排序，para1:被排序的欄位，para2: 升序True 降序False\n",
    "# first: DataFrame對象的API，取出DF的first row，回傳值是ROW對象\n",
    "# Row對象: 就是一個數組，可以透過row['欄位名']取出欄位的具體數值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb9db92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+\n",
      "| name| age|      job|\n",
      "+-----+----+---------+\n",
      "|Peter|  11|Developer|\n",
      "|Alice|   9|     null|\n",
      "|  Amy|  11|Developer|\n",
      "|Alice|   9|  Manager|\n",
      "|Alice|null|  Manager|\n",
      "| Lily|  11|  Manager|\n",
      "|jorge|  30|Developer|\n",
      "|  Bob|  32|Developer|\n",
      "+-----+----+---------+\n",
      "\n",
      "+-----+----+---------+\n",
      "| name| age|      job|\n",
      "+-----+----+---------+\n",
      "|jorge|  30|Developer|\n",
      "| Lily|  11|  Manager|\n",
      "|Alice|   9|  Manager|\n",
      "|Alice|   9|     null|\n",
      "|Alice|null|  Manager|\n",
      "|  Amy|  11|Developer|\n",
      "|  Bob|  32|Developer|\n",
      "+-----+----+---------+\n",
      "\n",
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "|  Amy| 11|Developer|\n",
      "| Lily| 11|  Manager|\n",
      "|Peter| 11|Developer|\n",
      "|Alice|  9|  Manager|\n",
      "|Alice|  9|  Manager|\n",
      "|Alice|  9|  Manager|\n",
      "+-----+---+---------+\n",
      "\n",
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "|  Amy| 11|Developer|\n",
      "| Lily| 11|  Manager|\n",
      "|Peter| 11|Developer|\n",
      "|Alice|  9|  Manager|\n",
      "|Alice|  9|  Manager|\n",
      "|Alice|  9|  Manager|\n",
      "+-----+---+---------+\n",
      "\n",
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "|  Amy| 11|Developer|\n",
      "| Lily| 11|  Manager|\n",
      "|Peter| 11|Developer|\n",
      "|Alice|  9|  Manager|\n",
      "|Alice|  9|  Manager|\n",
      "|Alice|  9|  Manager|\n",
      "|Alice|  9|     null|\n",
      "+-----+---+---------+\n",
      "\n",
      "+-----+----+---------+\n",
      "| name| age|      job|\n",
      "+-----+----+---------+\n",
      "|jorge|  30|Developer|\n",
      "|  Bob|  32|Developer|\n",
      "|  Amy|  11|Developer|\n",
      "| Lily|  11|  Manager|\n",
      "|Peter|  11|Developer|\n",
      "|Alice|   9|  Manager|\n",
      "|Alice|   9|  Manager|\n",
      "|Alice|   9|  Manager|\n",
      "|Alice|loss|  Manager|\n",
      "|Alice|   9|     loss|\n",
      "+-----+----+---------+\n",
      "\n",
      "+-----+----+---------+\n",
      "| name| age|      job|\n",
      "+-----+----+---------+\n",
      "|jorge|  30|Developer|\n",
      "|  Bob|  32|Developer|\n",
      "|  Amy|  11|Developer|\n",
      "| Lily|  11|  Manager|\n",
      "|Peter|  11|Developer|\n",
      "|Alice|   9|  Manager|\n",
      "|Alice|   9|  Manager|\n",
      "|Alice|   9|  Manager|\n",
      "|Alice|null|  Manager|\n",
      "|Alice|   9|      N/A|\n",
      "+-----+----+---------+\n",
      "\n",
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "|  Amy| 11|Developer|\n",
      "| Lily| 11|  Manager|\n",
      "|Peter| 11|Developer|\n",
      "|Alice|  9|  Manager|\n",
      "|Alice|  9|  Manager|\n",
      "|Alice|  9|  Manager|\n",
      "|Alice|  1|  Manager|\n",
      "|Alice|  9|   worker|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"sep\",\";\").option(\"header\",True).load(\"people.csv\")\n",
    "\n",
    "#去重複(對全部欄位)\n",
    "df.dropDuplicates().show()\n",
    "\n",
    "#針對特定欄位去重複\n",
    "df.dropDuplicates(['age','job']).show()\n",
    "\n",
    "#缺失值處理，有n/a值整列刪除\n",
    "df.dropna().show()\n",
    "df.dropna(thresh=3).show()\n",
    "df.dropna(thresh=2,subset=['name','age']).show()\n",
    "\n",
    "#將缺失值填充\n",
    "df.fillna(\"loss\").show()\n",
    "df.fillna(\"N/A\", subset=['job']).show()\n",
    "\n",
    "#設定字典對所有欄位加入填充規則\n",
    "df.fillna({\"name\":\"未知姓名\",\"age\":1,\"job\":\"worker\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5b9c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType().add(\"user_id\",StringType(),nullable=False).\\\n",
    "    add(\"movie_id\",IntegerType(),nullable=False).\\\n",
    "    add(\"rank\",IntegerType(),nullable=False).\\\n",
    "    add(\"ts\",StringType(),nullable=False)\n",
    "df = spark.read.format(\"csv\").option(\"sep\",\"\\t\").option(\"header\",False).option(\"encoding\",\"utf-8\").schema(schema).load(\"/home/cris/ml-100k/u.data\")\n",
    "\n",
    "# write text\n",
    "df.select(F.concat_ws(\"---\",\"user_id\",\"movie_id\",\"rank\",\"ts\")).write.mode(\"overwrite\").format(\"text\").save(\"/home/cris/Documents/text\")\n",
    "\n",
    "# csv\n",
    "df.write.mode(\"overwrite\").format(\"csv\").option(\"sep\", \";\").option(\"header\", True).save(\"/home/cris/Documents/csv\")\n",
    "\n",
    "# json\n",
    "df.write.mode(\"overwrite\").format(\"json\").save(\"/home/cris/Documents/json\")\n",
    "\n",
    "# parquet\n",
    "df.write.mode(\"overwrite\").format(\"parquet\").save(\"/home/cris/Documents/parquet\")\n",
    "\n",
    "# jdbc, need to install driver first\n",
    "df.write.mode(\"overwrite\").\\\n",
    "    format(\"jdbc\").\\\n",
    "    option(\"url\", \"jdbc:mysql://node1:3306/bigdata?useSSL=false&useUnicode=true\").\\\n",
    "    option(\"dbtable\", \"movie_data\").\\\n",
    "    option(\"user\", \"root\").\\\n",
    "    option(\"password\", \"123456\").\\\n",
    "    save()\n",
    "\n",
    "spark.read.format(\"jdbc\").\\\n",
    "    option(\"url\", \"jdbc:mysql://node1:3306/bigdata?useSSL=false&useUnicode=true\").\\\n",
    "    option(\"dbtable\", \"movie_data\").\\\n",
    "    option(\"user\", \"root\").\\\n",
    "    option(\"password\", \"123456\").\\\n",
    "    load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "946514da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|udf1(num)|\n",
      "+---------+\n",
      "|       10|\n",
      "|       20|\n",
      "|       30|\n",
      "|       40|\n",
      "|       50|\n",
      "|       60|\n",
      "|       70|\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|udf1(num)|\n",
      "+---------+\n",
      "|       10|\n",
      "|       20|\n",
      "|       30|\n",
      "|       40|\n",
      "|       50|\n",
      "|       60|\n",
      "|       70|\n",
      "+---------+\n",
      "\n",
      "+----------------+\n",
      "|num_ride_10(num)|\n",
      "+----------------+\n",
      "|              10|\n",
      "|              20|\n",
      "|              30|\n",
      "|              40|\n",
      "|              50|\n",
      "|              60|\n",
      "|              70|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5,6,7]).map(lambda x:[x])\n",
    "df = rdd.toDF([\"num\"])\n",
    "\n",
    "def num_ride_10(num):\n",
    "    return num * 10\n",
    "\n",
    "#1 sparksession.udf.register() DSL&SQL均可使用\n",
    "#para1: 創建的udf名稱，僅可以用於SQL\n",
    "#para2: 處理邏輯，是一個單獨的方法\n",
    "#para3: udf的回傳值類型，真實的回傳值一定要和聲明的一致\n",
    "#回傳值對象僅可以用於DSL風格\n",
    "udf2 = spark.udf.register(\"udf1\", num_ride_10, IntegerType())\n",
    "\n",
    "#SQL中使用 selectExpr:以SQL表達式執行\n",
    "df.selectExpr(\"udf1(num)\").show()\n",
    "\n",
    "#DSL中使用，注意欄位名稱還是被註冊的udf1\n",
    "df.select(udf2(df['num'])).show()\n",
    "\n",
    "#2 pyspark.sql.functions.udf，僅能用於DSL風格，名稱為函數名稱\n",
    "udf3 = F.udf(num_ride_10, IntegerType())\n",
    "df.select(udf3(df['num'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b44c6908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          udf1(line)|\n",
      "+--------------------+\n",
      "|[hadoop, spark, f...|\n",
      "|[hadoop, flink, j...|\n",
      "+--------------------+\n",
      "\n",
      "+----------------------+\n",
      "|udf1(line)            |\n",
      "+----------------------+\n",
      "|[hadoop, spark, flink]|\n",
      "|[hadoop, flink, java] |\n",
      "+----------------------+\n",
      "\n",
      "+--------------------+\n",
      "|    split_line(line)|\n",
      "+--------------------+\n",
      "|[hadoop, spark, f...|\n",
      "|[hadoop, flink, j...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([[\"hadoop spark flink\"],[\"hadoop flink java\"]])\n",
    "df = rdd.toDF([\"line\"])\n",
    "\n",
    "def split_line(data):\n",
    "    return data.split(\" \") #回傳值是Array\n",
    "\n",
    "udf2 = spark.udf.register(\"udf1\", split_line, ArrayType(StringType()))\n",
    "#DSL風格\n",
    "df.select(udf2(df['line'])).show()\n",
    "#SQL風格\n",
    "df.createOrReplaceTempView(\"lines\")\n",
    "spark.sql(\"SELECT udf1(line) FROM lines\").show(truncate=False)\n",
    "\n",
    "udf3 = F.udf(split_line, ArrayType(StringType()))\n",
    "df.select(udf3(df['line'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0f266a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|udf1(num)|\n",
      "+---------+\n",
      "|{1, b}   |\n",
      "|{2, c}   |\n",
      "|{3, d}   |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|udf1(num)|\n",
      "+---------+\n",
      "|{1, b}   |\n",
      "|{2, c}   |\n",
      "|{3, d}   |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([[1],[2],[3]])\n",
    "df = rdd.toDF([\"num\"])\n",
    "\n",
    "def process(data):\n",
    "    return {\"num\": data, \"letters\": string.ascii_letters[data]}\n",
    "\n",
    "#udf回傳值是字典的話要用StructType來實作\n",
    "\n",
    "udf1 = spark.udf.register(\"udf1\", process, StructType().add(\"num\", IntegerType(), nullable=True).add(\"letters\", StringType(), nullable=True))\n",
    "df.selectExpr(\"udf1(num)\").show(truncate=False)\n",
    "df.select(udf1(df['num'])).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b2310d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1 , 2, 3, 4, 5], 3)\n",
    "df = rdd.map(lambda x: [x]).toDF(['num'])\n",
    "\n",
    "# 折衷方式達成UDAF 利用RDD算子達到聚合的操作\n",
    "# 從DataFrame進行repartition回傳是row對象-->因為是從DataFrame去重新分區\n",
    "single_partition_rdd = df.rdd.repartition(1)\n",
    "\n",
    "def process(iter):\n",
    "    sum = 0\n",
    "    for row in iter:\n",
    "        sum += row['num']\n",
    "    return [sum]  # 這裡要回傳list，因為mapPartition方法支持的是list對象\n",
    "\n",
    "print(single_partition_rdd.mapPartitions(process).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3f54b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+-----------------+\n",
      "|name|class|score|        avg_score|\n",
      "+----+-----+-----+-----------------+\n",
      "|   1| 國語|   90|92.33333333333333|\n",
      "|   2| 數學|   91|92.33333333333333|\n",
      "|   3| 英語|   92|92.33333333333333|\n",
      "|   2| 英語|   96|92.33333333333333|\n",
      "|   2| 國語|   91|92.33333333333333|\n",
      "|   1| 數學|   94|92.33333333333333|\n",
      "|   3| 數學|   95|92.33333333333333|\n",
      "|   1| 英語|   91|92.33333333333333|\n",
      "|   3| 國語|   91|92.33333333333333|\n",
      "+----+-----+-----+-----------------+\n",
      "\n",
      "+----+-----+-----+---------------+----------+----+\n",
      "|name|class|score|row_number_rank|dense_rank|rank|\n",
      "+----+-----+-----+---------------+----------+----+\n",
      "|   2| 國語|   91|              6|         1|   2|\n",
      "|   3| 國語|   91|              8|         1|   2|\n",
      "|   1| 國語|   90|              9|         2|   1|\n",
      "|   3| 數學|   95|              2|         1|   8|\n",
      "|   1| 數學|   94|              3|         2|   7|\n",
      "|   2| 數學|   91|              5|         3|   2|\n",
      "|   2| 英語|   96|              1|         1|   9|\n",
      "|   3| 英語|   92|              4|         2|   6|\n",
      "|   1| 英語|   91|              7|         3|   2|\n",
      "+----+-----+-----+---------------+----------+----+\n",
      "\n",
      "+----+-----+-----+-----------------------------------------------------------------------------------------------+\n",
      "|name|class|score|ntile(6) OVER (ORDER BY score DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|\n",
      "+----+-----+-----+-----------------------------------------------------------------------------------------------+\n",
      "|   2| 英語|   96|                                                                                              1|\n",
      "|   3| 數學|   95|                                                                                              1|\n",
      "|   1| 數學|   94|                                                                                              2|\n",
      "|   3| 英語|   92|                                                                                              2|\n",
      "|   2| 數學|   91|                                                                                              3|\n",
      "|   2| 國語|   91|                                                                                              3|\n",
      "|   1| 英語|   91|                                                                                              4|\n",
      "|   3| 國語|   91|                                                                                              5|\n",
      "|   1| 國語|   90|                                                                                              6|\n",
      "+----+-----+-----+-----------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(1,\"國語\",90),(2,\"數學\",91),(3,\"英語\",92),(2,\"英語\",96),(2,\"國語\",91),(1,\"數學\",94),(3,\"數學\",95),(1,\"英語\",91),(3,\"國語\",91)])\n",
    "schema = StructType().add(\"name\", StringType()).add(\"class\", StringType()).add(\"score\", IntegerType())\n",
    "df = rdd.toDF(schema)\n",
    "\n",
    "df.createOrReplaceTempView(\"stu\")\n",
    "\n",
    "# 聚合窗口函數\n",
    "spark.sql(\"SELECT *, AVG(score) OVER() AS avg_score  FROM stu\").show()\n",
    "\n",
    "# 排序窗口函數\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER(ORDER BY score DESC) AS row_number_rank, \n",
    "    DENSE_RANK() OVER(PARTITION BY class ORDER BY score DESC) AS dense_rank,\n",
    "    RANK() OVER(ORDER BY score) AS rank\n",
    "    FROM stu\n",
    "    \"\"\").show()\n",
    "\n",
    "# NTILE分組窗口函數\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *, NTILE(6) OVER(ORDER BY score DESC) FROM stu\n",
    "    \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ea7fa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting koalas\n",
      "  Downloading koalas-1.8.2-py3-none-any.whl (390 kB)\n",
      "\u001b[K     |████████████████████████████████| 390 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=0.10 in /opt/conda/lib/python3.8/site-packages (from koalas) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.8/site-packages (from koalas) (1.20.2)\n",
      "Requirement already satisfied: pandas>=0.23.2 in /opt/conda/lib/python3.8/site-packages (from koalas) (1.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23.2->koalas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23.2->koalas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=0.23.2->koalas) (1.15.0)\n",
      "Installing collected packages: koalas\n",
      "Successfully installed koalas-1.8.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install koalas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44536b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import databricks.koalas as ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcd1afc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-0.751916</th>\n",
       "      <td>0.332331</td>\n",
       "      <td>-2.377288</td>\n",
       "      <td>1.663634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-2.000716</th>\n",
       "      <td>0.914490</td>\n",
       "      <td>-1.668963</td>\n",
       "      <td>-1.132850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-0.514297</th>\n",
       "      <td>0.937845</td>\n",
       "      <td>0.183965</td>\n",
       "      <td>2.681256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.026387</th>\n",
       "      <td>1.104478</td>\n",
       "      <td>-1.312707</td>\n",
       "      <td>-0.134974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-0.172941</th>\n",
       "      <td>1.110820</td>\n",
       "      <td>0.477678</td>\n",
       "      <td>1.788781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-0.896663</th>\n",
       "      <td>-0.741169</td>\n",
       "      <td>-0.072685</td>\n",
       "      <td>1.004584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  B         C         D\n",
       "A                                      \n",
       "-0.751916  0.332331 -2.377288  1.663634\n",
       "-2.000716  0.914490 -1.668963 -1.132850\n",
       "-0.514297  0.937845  0.183965  2.681256\n",
       " 0.026387  1.104478 -1.312707 -0.134974\n",
       "-0.172941  1.110820  0.477678  1.788781\n",
       "-0.896663 -0.741169 -0.072685  1.004584"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = pd.date_range('20130101', periods=6)\n",
    "pdf = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))\n",
    "kdf = ks.from_pandas(pdf) #底層跑spark的分布式計算\n",
    "type(kdf)\n",
    "sdf = spark.createDataFrame(pdf)\n",
    "kdf = sdf.to_koalas()\n",
    "kdf\n",
    "ks.DataFrame({'A':['foo', 'bar'], 'B': [1, 2]})\n",
    "kdf.head()\n",
    "kdf.index\n",
    "kdf.columns\n",
    "kdf.to_numpy()\n",
    "kdf.describe()\n",
    "kdf.T\n",
    "pdf1 = pdf.reindex(index=dates[0:4], columns=list(pdf.columns)+['E'])\n",
    "pdf1.loc[dates[0]:dates[1], 'E'] = 1\n",
    "pdf1\n",
    "kdf1 = ks.from_pandas(pdf1)\n",
    "kdf1.dropna(how='any')\n",
    "kdf1.fillna(value=5)\n",
    "kdf.groupby('A').sum()\n",
    "kdf.to_csv('foo.csv')\n",
    "ks.read_csv('foo.csv').head(10)\n",
    "kdf.to_parquet('bar.parquet')\n",
    "ks.read_parquet('bar.parquet')\n",
    "kdf.to_spark_io('zoo.orc', format=\"orc\") #調用spark io\n",
    "ks.read_spark_io('zoo.orc', format=\"orc\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b0e01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
