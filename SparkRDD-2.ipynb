{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aed8b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import jieba\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f896aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"test\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab879e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.362 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['小', '明', '碩', '士', '畢', '業', '於', '建', '國', '中', '學', '後', '在', '台', '灣', '大', '學', '就', '讀']\n",
      "<class 'generator'>\n"
     ]
    }
   ],
   "source": [
    "content = \"小明碩士畢業於建國中學後在台灣大學就讀\"\n",
    "result = jieba.cut(content,True)\n",
    "print(list(result))\n",
    "print(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce1ec3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('地震', 615), ('的', 492), ('汶川', 433), ('原因', 360), ('救灾', 338)]\n"
     ]
    }
   ],
   "source": [
    "# read file \n",
    "file_rdd = sc.textFile(\"SogouQ.txt\")\n",
    "\n",
    "#split file\n",
    "split_rdd = file_rdd.map(lambda x: x.split(\"\\t\"))\n",
    "\n",
    "# cus split_rdd need to use more than one time, so we cache it.\n",
    "split_rdd.persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "# analysis user search \n",
    "# print(split_rdd.takeSample(True,3))\n",
    "def context_jieba(data):\n",
    "    # 使用jieba分詞\n",
    "    seg = jieba.cut_for_search(data)\n",
    "    l = list()\n",
    "    for word in seg:\n",
    "        l.append(word)\n",
    "    return l\n",
    "\n",
    "# 取出搜索詞並括號取代掉\n",
    "context_rdd = split_rdd.map(lambda x: x[2].replace('[','').replace(']','').replace('+','').replace('.',''))\n",
    "\n",
    "# 使用flatMap是為了解決分詞後會分很多組的問題\n",
    "words_rdd = context_rdd.flatMap(context_jieba)\n",
    "\n",
    "#print(words_rdd.collect())\n",
    "\n",
    "#用map變成數組用以統計\n",
    "final_words_rdd = words_rdd.map(lambda x:(x, 1))\n",
    "\n",
    "#取出前N名,照理說要先進行資料清理把贅字符號移除\n",
    "print(final_words_rdd.reduceByKey(lambda a, b: a + b).sortBy(lambda x: x[1], ascending=False, numPartitions=1).take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ddc7dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1011517038707826_主题', 27), ('7230120314300312_阿宾', 23), ('7230120314300312_全集', 21), ('7230120314300312_阅读', 20), ('9026201537815861_scat', 19)]\n"
     ]
    }
   ],
   "source": [
    "#用戶關鍵詞組合分析\n",
    "user_content_rdd = split_rdd.map(lambda x:(x[1],x[2]))\n",
    "#對user搜尋進行分詞再根據ID組合\n",
    "def extract_user_and_word(data):\n",
    "    user_id = data[0]\n",
    "    content = data[1].replace('[','').replace(']','').replace('+','').replace('.','')\n",
    "    words = context_jieba(content)\n",
    "    return_list = list()\n",
    "    for word in words:\n",
    "        return_list.append(user_id + \"_\" + word)\n",
    "    return return_list\n",
    "\n",
    "user_word_with_one_rdd = user_content_rdd.flatMap(extract_user_and_word)\n",
    "\n",
    "\n",
    "#分組聚合排序\n",
    "result = user_word_with_one_rdd.map(lambda x: (x,1)).\\\n",
    "    reduceByKey(lambda a, b: a + b).\\\n",
    "    sortBy(lambda x: x[1],ascending=False,numPartitions=1).\\\n",
    "    take(5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2491b4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('02', 1088), ('04', 1056), ('03', 1051), ('00', 1046), ('01', 1046), ('06', 1036), ('05', 1024), ('08', 1024), ('07', 999), ('09', 630)]\n"
     ]
    }
   ],
   "source": [
    "#熱門搜索時間段分析\n",
    "time_rdd = split_rdd.map(lambda x: x[0])\n",
    "\n",
    "#保留分精度\n",
    "minute_with_one_rdd = time_rdd.map(lambda x: (x.split(\":\")[1],1))\n",
    "\n",
    "#分組 聚合\n",
    "minute_result = minute_with_one_rdd.reduceByKey(add).\\\n",
    "    sortBy(lambda x: x[1],ascending=False,numPartitions=1).\\\n",
    "    collect()\n",
    "print(minute_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d131568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Amy', '國語', 90), ('Bob', '數學', 91), ('Chris', '英語', 92), ('Bob', '英語', 96), ('Bob', '國語', 91), ('Amy', '數學', 94), ('Chris', '數學', 95), ('Amy', '英語', 91), ('Chris', '國語', 91)]\n",
      "[('Amy', '國語', 90), ('Bob', '數學', 91), ('Chris', '英語', 92), ('Bob', '英語', 96), ('Bob', '國語', 91), ('Amy', '數學', 94), ('Chris', '數學', 95), ('Amy', '英語', 91), ('Chris', '國語', 91)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "stu_info_list=[(1,\"Amy\",11),(2,\"Bob\",13),(3,\"Chris\",11),(4,\"David\",11)]\n",
    "score_info_rdd = sc.parallelize([(1,\"國語\",90),(2,\"數學\",91),(3,\"英語\",92),(2,\"英語\",96),(2,\"國語\",91),(1,\"數學\",94),(3,\"數學\",95),(1,\"英語\",91),(3,\"國語\",91)])\n",
    "\n",
    "# 未使用廣播變量\n",
    "\n",
    "def map_func(data):\n",
    "    id = data[0]\n",
    "    name = \"\"\n",
    "    for stu_info in stu_info_list:\n",
    "        stu_id = stu_info[0]\n",
    "        if id == stu_id:\n",
    "            name = stu_info[1]\n",
    "    return (name,data[1],data[2])\n",
    "\n",
    "\n",
    "print(score_info_rdd.map(map_func).collect())\n",
    "\n",
    "# 使用廣播變量\n",
    "\n",
    "broadcast = sc.broadcast(stu_info_list)\n",
    "\n",
    "def map_func(data):\n",
    "    id = data[0]\n",
    "    name = \"\"\n",
    "    for stu_info in broadcast.value:\n",
    "        stu_id = stu_info[0]\n",
    "        if id == stu_id:\n",
    "            name = stu_info[1]\n",
    "    return (name,data[1],data[2])\n",
    "\n",
    "print(score_info_rdd.map(map_func).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e08ac8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10], 2)\n",
    "\n",
    "count = 0\n",
    "\n",
    "def map_func(data):\n",
    "    global count\n",
    "    count += 1\n",
    "    print(count)\n",
    "    \n",
    "rdd.map(map_func).collect()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d184df3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10], 2)\n",
    "\n",
    "count = sc.accumulator(0)\n",
    "\n",
    "def map_func(data):\n",
    "    global count\n",
    "    count += 1\n",
    "    print(count)\n",
    "    \n",
    "rdd.map(map_func).collect()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811b1f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
